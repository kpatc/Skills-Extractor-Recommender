# Plateforme Intelligente d'Extraction de CompÃ©tences Tech

## ğŸ“‹ Vue d'ensemble

**Extraction et analyse automatique des compÃ©tences techniques** Ã  partir des offres d'emploi marocaines et internationales.

### Objectif Principal
Automatiser la collecte, l'analyse et la valorisation des compÃ©tences demandÃ©es pour aider :
- âœ… **Ã‰tudiants** : identifier les compÃ©tences Ã  apprendre
- âœ… **Ã‰coles** : aligner les cursus avec le marchÃ©
- âœ… **Recruteurs** : mieux comprendre le marchÃ© des talents

---

## ğŸ—ï¸ Architecture du Projet

### Pipeline de Traitement

```
[Sources Web]
    â†“
[Scrapers Python]
    â†“
[Raw Data - JSON/CSV]
    â†“
[Preprocessing NLP]
    â†“
[Extraction des compÃ©tences]
    â†“
[Vectorisation / Embeddings]
    â†“
[Clustering]
    â†“
[Recommandation]
    â†“
[Dashboard / API]
```

### Structure des RÃ©pertoires

```
skill_extractor/
â”œâ”€â”€ scrapping/          # Module de scraping des offres
â”‚   â””â”€â”€ scraper.py
â”œâ”€â”€ nlp/                # Module NLP & nettoyage
â”‚   â”œâ”€â”€ text_cleaner.py
â”‚   â””â”€â”€ skills_extractor.py
â”œâ”€â”€ modelling/          # Module clustering & vectorisation
â”‚   â””â”€â”€ clustering.py
â”œâ”€â”€ recommendtion/      # Module recommandation
â”‚   â””â”€â”€ recommender.py
â”œâ”€â”€ data/               # DonnÃ©es
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ models/             # ModÃ¨les sauvegardÃ©s
â”œâ”€â”€ utils/              # Utilitaires
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ pipeline.py         # Orchestration complÃ¨te
â”œâ”€â”€ test_pipeline.py    # Tests
â”œâ”€â”€ setup_env.py        # Configuration
â””â”€â”€ requirements.txt    # DÃ©pendances
```

---

## ğŸ”§ Installation et Configuration

### 1. PrÃ©requis
- Python 3.8+
- pip (gestionnaire de paquets)
- Git

### 2. Installation des dÃ©pendances

```bash
# Cloner le projet (si applicable)
cd skill_extractor

# Installer les paquets
pip install -r requirements.txt

# Configurer l'environnement
python setup_env.py
```

Cela tÃ©lÃ©chargera automatiquement les modÃ¨les spaCy nÃ©cessaires.

### 3. VÃ©rification

```bash
# ExÃ©cuter les tests
pytest test_pipeline.py -v
```

---

## ğŸ“Š Modules DÃ©taillÃ©s

### Module 1: Scraping (`scrapping/scraper.py`)

**ResponsabilitÃ©** : Collecter les offres d'emploi

**Sources supportÃ©es** :
- ğŸ‡²ğŸ‡¦ Marocaines: ReKrute, Emploi.ma
- ğŸŒ Internationales: Indeed, Glassdoor, LinkedIn

**FonctionnalitÃ©s** :
- âœ… Pagination automatique
- âœ… Gestion des doublons
- âœ… Extraction du texte complet
- âœ… Gestion des erreurs et retry

**Structure d'une offre** :
```python
{
    "job_id": "rekrute_001",
    "title": "Data Engineer",
    "company": "TechCorp",
    "location": "Casablanca",
    "description": "We are looking for...",
    "source": "rekrute",
    "scrape_date": "2024-01-10"
}
```

**Utilisation** :
```python
from skill_extractor.scrapping.scraper import scrape_all_sources

# Mode test (donnÃ©es simulÃ©es)
offers = scrape_all_sources(test_mode=True)

# Mode production (scraping rÃ©el)
offers = scrape_all_sources(test_mode=False)
```

---

### Module 2: NLP et Nettoyage (`nlp/text_cleaner.py`)

**ResponsabilitÃ©** : Nettoyer et prÃ©parer les textes

**Ã‰tapes de nettoyage** :
1. Suppression du HTML
2. Suppression des URLs et emails
3. Conversion en minuscules
4. Suppression des caractÃ¨res spÃ©ciaux
5. Suppression des espaces superflus
6. Lemmatisation (optionnel)
7. Suppression des stopwords (optionnel)

**Utilisation** :
```python
from skill_extractor.nlp.text_cleaner import TextCleaner

cleaner = TextCleaner()

# Nettoyer un texte
cleaned = cleaner.clean(
    "<p>Senior Python Developer</p>",
    lemmatize=True,
    remove_stopwords=True
)

# Nettoyer plusieurs offres
cleaned_offers = cleaner.clean_job_offers(offers)
```

---

### Module 3: Extraction des CompÃ©tences (`nlp/skills_extractor.py`)

**ResponsabilitÃ©** : Extraire les compÃ©tences techniques

**Deux approches combinÃ©es** :

#### A. Dictionnaire + Regex
- âœ… Rapide et fiable
- âŒ NÃ©cessite un dictionnaire Ã  jour
- Couverture: `TECH_SKILLS` (config.py)

```python
extractor = SkillExtractor()
skills_set = extractor.extract_skills_regex(text)
# â†’ {"python", "sql", "docker"}
```

#### B. Extraction SÃ©mantique
- âœ… DÃ©tecte les variations
- âŒ Plus lent, nÃ©cessite GPU pour performance
- Utilise: sentence-transformers + similaritÃ© cosinus

```python
skills_data = extractor.extract_skills(
    text,
    method="semantic"  # ou "regex", "spacy", "combined"
)
# â†’ {
#     "skills": ["python", "sql"],
#     "count": 2,
#     "categorized": {...}
# }
```

**CatÃ©gories de compÃ©tences** :
- ğŸ Languages: python, java, javascript, go, rust...
- ğŸ“Š Data & ML: sql, spark, tensorflow, pytorch...
- â˜ï¸ DevOps: docker, kubernetes, aws, azure...
- ğŸ¨ Frontend: react, vue, angular...
- ğŸ”§ Backend: nodejs, fastapi, django, spring...

---

### Module 4: Vectorisation & Clustering (`modelling/clustering.py`)

**ResponsabilitÃ©** : Regrouper les offres par profil

#### Vectorisation

```python
from skill_extractor.modelling.clustering import SkillsVectorizer

vectorizer = SkillsVectorizer()
embeddings = vectorizer.vectorize_descriptions(offers)
# â†’ shape: (n_offers, embedding_dim)
```

**ModÃ¨les disponibles** :
- Sentence-Transformers (par dÃ©faut, recommandÃ©)
- TF-IDF (fallback)

#### Clustering

```python
from skill_extractor.modelling.clustering import OffersClustering

clusterer = OffersClustering()
clusterer.fit(embeddings)

# Clusters: 0=Data, 1=Backend, 2=DevOps, 3=AI/ML, 4=Frontend
```

**Algorithmes** :
- K-Means (simple, pÃ©dagogique)
- HDBSCAN (dÃ©tecte clusters de formes arbitraires)

---

### Module 5: Recommandation (`recommendtion/recommender.py`)

**ResponsabilitÃ©** : GÃ©nÃ©rer des recommandations personnalisÃ©es

**Profils supportÃ©s** :
- ğŸ”¢ `data_engineer`: Python, SQL, Spark, AWS
- ğŸ–¥ï¸ `backend_dev`: Node.js, PostgreSQL, Docker
- âš™ï¸ `devops_engineer`: Docker, Kubernetes, CI/CD
- ğŸ¤– `ml_engineer`: TensorFlow, PyTorch, NLP
- ğŸ¨ `frontend_dev`: React, TypeScript, Tailwind

**Recommandations par profil** :

```python
from skill_extractor.recommendtion.recommender import SkillRecommender

recommender = SkillRecommender()

# Recommandation personnalisÃ©e
rec = recommender.recommend_for_profile(
    profile_name="data_engineer",
    cluster_data=clustering_result,
    top_n=15
)

# RÃ©sultat:
# {
#     "profile": "data_engineer",
#     "current_skills": ["python", "sql", ...],
#     "recommended_skills": [
#         {
#             "skill": "spark",
#             "frequency": 45,
#             "importance_score": 0.92,
#             "priority": 67.5
#         },
#         ...
#     ],
#     "learning_path": [
#         {
#             "phase": "Fondamentaux (0-3 mois)",
#             "skills": [...]
#         },
#         ...
#     ]
# }
```

**Analyse de l'Ã©cart** :

```python
gap = recommender.get_skills_gap("data_engineer", cluster_data)
# â†’ {
#     "missing_skills": [...],
#     "gap_percentage": 45.2
# }
```

---

## ğŸš€ ExÃ©cution du Pipeline

### Mode Test (RecommandÃ© pour dÃ©marrer)

```python
from skill_extractor.pipeline import main

# ExÃ©cuter le pipeline complet
result = main(test_mode=True)

print(f"Offres traitÃ©es: {result['offers_raw_count']}")
print(f"Recommandations gÃ©nÃ©rÃ©es: {len(result['recommendations'])}")
```

### Mode Production

```python
from skill_extractor.pipeline import SkillExtractionPipeline

pipeline = SkillExtractionPipeline(test_mode=False)
result = pipeline.run_full_pipeline()

# Les donnÃ©es sont sauvegardÃ©es dans:
# - data/raw/job_offers_raw.csv
# - data/processed/job_offers_cleaned.csv
# - data/processed/recommendations.json
```

### ExÃ©cution Step-by-Step

```python
from skill_extractor.scrapping.scraper import scrape_all_sources
from skill_extractor.nlp.text_cleaner import clean_offers_pipeline
from skill_extractor.nlp.skills_extractor import extract_skills_pipeline
from skill_extractor.modelling.clustering import cluster_offers
from skill_extractor.recommendtion.recommender import generate_recommendations_pipeline

# Ã‰tape 1: Scraping
offers = scrape_all_sources(test_mode=True)

# Ã‰tape 2: Nettoyage
offers = clean_offers_pipeline(offers)

# Ã‰tape 3: Extraction des compÃ©tences
offers = extract_skills_pipeline(offers)

# Ã‰tape 4: Clustering
clustering_result = cluster_offers(offers)

# Ã‰tape 5: Recommandations
recommendations = generate_recommendations_pipeline(clustering_result)

# Afficher les rÃ©sultats
for profile, data in recommendations.items():
    print(f"\n{profile}:")
    for rec in data['recommended_skills'][:5]:
        print(f"  - {rec['skill']}: {rec['priority']:.1f}")
```

---

## ğŸ“ˆ RÃ©sultats et Sortie

### Fichiers gÃ©nÃ©rÃ©s

```
skill_extractor/data/
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ job_offers_raw.csv           # Offres brutes
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ job_offers_cleaned.csv       # Offres nettoyÃ©es
â”‚   â”œâ”€â”€ skills_embeddings.npy        # Embeddings vectorisÃ©s
â”‚   â””â”€â”€ recommendations.json         # Recommandations
â””â”€â”€ models/
    â””â”€â”€ clustering_model.pkl         # ModÃ¨le clustering sauvegardÃ©
```

### Format des recommandations (JSON)

```json
{
  "data_engineer": {
    "profile": "data_engineer",
    "cluster": "Data",
    "current_skills": ["python", "sql", "spark", "docker", "aws"],
    "recommended_skills": [
      {
        "skill": "apache_spark",
        "frequency": 45,
        "importance_score": 0.92,
        "already_core": false,
        "priority": 67.5
      }
    ],
    "learning_path": [
      {
        "phase": "Fondamentaux (0-3 mois)",
        "skills": ["apache_spark", "airflow"],
        "description": "..."
      }
    ]
  }
}
```

---

## ğŸ§ª Tests

### ExÃ©cuter tous les tests

```bash
pytest test_pipeline.py -v
```

### Tests spÃ©cifiques

```bash
# Tests de scraping
pytest test_pipeline.py::TestScraping -v

# Tests de NLP
pytest test_pipeline.py::TestTextCleaning -v

# Tests d'extraction des compÃ©tences
pytest test_pipeline.py::TestSkillExtraction -v

# Tests du pipeline complet
pytest test_pipeline.py::TestPipeline -v
```

### Couverture de tests

```bash
pytest test_pipeline.py --cov=skill_extractor --cov-report=html
```

---

## ğŸ“š Configuration AvancÃ©e

### Personnaliser les compÃ©tences

Modifier `utils/config.py` :

```python
TECH_SKILLS = {
    "languages": ["python", "java", "go", ...],
    "data_and_ml": ["sql", "spark", ...],
    # ...
}
```

### ParamÃ¨tres de clustering

```python
CLUSTERING_CONFIG = {
    "algorithm": "kmeans",  # ou "hdbscan"
    "n_clusters": 5,
    "random_state": 42,
}
```

### ParamÃ¨tres NLP

```python
NLP_CONFIG = {
    "model_name": "sentence-transformers/multilingual-MiniLM-L12-v2",
    "language": "fr",
    "remove_stopwords": True,
    "lemmatization": True,
}
```

---

## ğŸ” DÃ©pannage

### Erreur: "ModuleNotFoundError: No module named 'spacy'"

```bash
pip install -r requirements.txt
python setup_env.py
```

### Erreur: "OSError: [E050] Can't find model 'fr_core_news_sm'"

```bash
python -m spacy download fr_core_news_sm
```

### Erreur: "ImportError: No module named 'sentence_transformers'"

```bash
pip install sentence-transformers
```

### Performance lente

- Utilisez `test_mode=True` pour dÃ©veloppement
- Activez GPU: `export CUDA_VISIBLE_DEVICES=0`
- RÃ©duisez le nombre de clusters

---

## ğŸ“Š Cas d'Usage

### Cas 1: Analyser le marchÃ© data

```python
from skill_extractor.pipeline import SkillExtractionPipeline

pipeline = SkillExtractionPipeline(test_mode=False)
result = pipeline.run_full_pipeline()

# Voir les compÃ©tences les plus demandÃ©es
from skill_extractor.nlp.skills_extractor import extractor
top_skills = extractor.get_top_skills(result["offers_with_skills"], top_n=20)

for skill, count in top_skills:
    print(f"{skill}: {count} offres")
```

### Cas 2: Obtenir des recommandations personnalisÃ©es

```python
pipeline = SkillExtractionPipeline(test_mode=True)
result = pipeline.run_full_pipeline()

recommender_data = result["recommendations"]
data_eng_recommendations = recommender_data["data_engineer"]

print("CompÃ©tences recommandÃ©es:")
for rec in data_eng_recommendations["recommended_skills"][:5]:
    print(f"- {rec['skill']} (prioritÃ©: {rec['priority']})")

print("\nChemin d'apprentissage:")
for phase in data_eng_recommendations["learning_path"]:
    print(f"\n{phase['phase']}")
    for skill in phase['skills']:
        print(f"  - {skill}")
```

### Cas 3: Suivre l'Ã©volution du marchÃ©

```python
from datetime import datetime, timedelta

# Scraper les offres chaque semaine
for i in range(4):
    offers = scrape_all_sources(test_mode=False)
    # Traiter et sauvegarder
    pipeline = SkillExtractionPipeline()
    pipeline.offers_raw = offers
    # ...

# Analyser les tendances
```

---

## ğŸ” ConsidÃ©rations Ã‰thiques

- âœ… Respecter le `robots.txt` des sites
- âœ… Respecter les dÃ©lais entre requÃªtes (2s par dÃ©faut)
- âœ… Ne pas surcharger les serveurs
- âœ… Utiliser un User-Agent appropriÃ©
- âœ… Anonymiser les donnÃ©es personnelles

---

## ğŸš€ Prochaines Ã‰tapes

### Court terme (v1.1)
- [ ] API REST FastAPI
- [ ] Dashboard Streamlit
- [ ] Support de base de donnÃ©es (PostgreSQL)

### Moyen terme (v1.2)
- [ ] Fine-tuning sur donnÃ©es marocaines
- [ ] Exportation Excel/PDF
- [ ] Webhooks pour notifications

### Long terme (v2.0)
- [ ] ML en continu (online learning)
- [ ] PrÃ©dictions salariales
- [ ] Recommandations par rÃ©gion

---

## ğŸ“ Licence

Ce projet est fourni Ã  titre Ã©ducatif.

---

## ğŸ“ Support

Pour toute question ou problÃ¨me:
1. VÃ©rifier la documentation
2. Consulter les tests
3. Examiner les logs

---

## ğŸ“š Ressources Externes

- [spaCy Documentation](https://spacy.io/)
- [Sentence-Transformers](https://www.sbert.net/)
- [scikit-learn](https://scikit-learn.org/)
- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/)

---

**CrÃ©Ã© avec â¤ï¸ pour le succÃ¨s des dÃ©veloppeurs marocains et internationaux**
